# -*- coding: utf-8 -*-
"""splits.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IIxO02ErW03rkUyOWHA_gRfL2RMXPASv

# Count frequencies (Local Run)
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import mean_squared_error
import os

"""## Iterate to find the best number of bins and weighted sum"""

df = pd.read_csv("")

# Define split sizes (70:20:10)
train_ratio = 0.7  # 70% for training
dev_ratio = 0.1    # 10% for development (dev)
test_ratio = 0.2   # 20% for testing

# Sanity check: calculate exact numbers of files for each split
total_files = len(df)
n_train = int(train_ratio * total_files)
n_dev = int(dev_ratio * total_files)
n_test = total_files - n_train - n_dev

print(f"Total files: {total_files}")
print(f"Training set size: {n_train} files ({train_ratio*100:.1f}%)")
print(f"Development set size: {n_dev} files ({dev_ratio*100:.1f}%)")
print(f"Test set size: {n_test} files ({test_ratio*100:.1f}%)")

# Get all entity columns
entity_columns = [col for col in df.columns if col not in ['Filename', 'Total', 'stratify_bin', 'weighted_sum']]

# Use StratifiedKFold to split directly into three sets
def perform_split(df, stratify_col, n_bins, method_name):
    # Create bins
    if n_bins > 1:
        stratify_bin = pd.qcut(df[stratify_col], q=n_bins, labels=False, duplicates='drop')
    else:
        stratify_bin = pd.Series([0] * len(df))  # No stratification

    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

    train_files = []
    dev_files = []
    test_files = []

    # Perform a single stratified split
    for train_dev_index, test_index in skf.split(df, stratify_bin):
        train_dev_df = df.iloc[train_dev_index]
        test_df = df.iloc[test_index]

        # Shuffle train_dev_df for randomness
        train_dev_df = train_dev_df.sample(frac=1, random_state=42)

        # Calculate sizes for train and dev from train_dev
        train_dev_size = len(train_dev_df)
        n_train_from_dev = int((train_ratio / (train_ratio + dev_ratio)) * train_dev_size)

        train_files.extend(train_dev_df['Filename'].iloc[:n_train_from_dev].tolist())
        dev_files.extend(train_dev_df['Filename'].iloc[n_train_from_dev:].tolist())
        test_files.extend(test_df['Filename'].tolist())
        break

    # Ensure no duplicates/correct total
    all_files = set(train_files + dev_files + test_files)
    if len(all_files) != total_files:
        print(f"Warning: {method_name} - Duplicate files detected, adjusting...")
        # Rebuilding splits
        remaining_files = list(set(df['Filename'].tolist()) - all_files)
        while remaining_files:
            if len(train_files) < n_train:
                train_files.append(remaining_files.pop(0))
            elif len(dev_files) < n_dev:
                dev_files.append(remaining_files.pop(0))
            elif len(test_files) < n_test:
                test_files.append(remaining_files.pop(0))

    train_df = df[df['Filename'].isin(train_files)]
    dev_df = df[df['Filename'].isin(dev_files)]
    test_df = df[df['Filename'].isin(test_files)]

    return train_df, dev_df, test_df

# Calculate distribution similarity (MSE)
def evaluate_distribution(original_df, train_df, dev_df, test_df, method_name, entity_cols):
    # Original distribution for all entities
    original_dist = original_df[entity_cols].sum() / len(original_df)

    # Distributions for each split
    train_dist = train_df[entity_cols].sum() / len(train_df)
    dev_dist = dev_df[entity_cols].sum() / len(dev_df)
    test_dist = test_df[entity_cols].sum() / len(test_df)

    # MSE between original and each split
    mse_train = mean_squared_error(original_dist, train_dist)
    mse_dev = mean_squared_error(original_dist, dev_dist)
    mse_test = mean_squared_error(original_dist, test_dist)

    avg_mse = (mse_train + mse_dev + mse_test) / 3

    print(f"\n{method_name}:")
    print(f"Training Set Distribution (Top 5 Entities):")
    print(train_df[entity_cols].sum().sort_values(ascending=False).head(5))
    print(f"Development Set Distribution (Top 5 Entities):")
    print(dev_df[entity_cols].sum().sort_values(ascending=False).head(5))
    print(f"Test Set Distribution (Top 5 Entities):")
    print(test_df[entity_cols].sum().sort_values(ascending=False).head(5))
    print(f"Average MSE (distribution similarity to original): {avg_mse:.6f}")
    return avg_mse

# Testing different stratification strategies
results = {}

# Vary number of bins for Total
for n_bins in [5, 10, 15, 20]:
    method_name = f"Total_q={n_bins}"
    train_df, dev_df, test_df = perform_split(df, 'Total', n_bins, method_name)
    mse = evaluate_distribution(df, train_df, dev_df, test_df, method_name, entity_columns)
    results[method_name] = mse

# Stratify on specific entity columns
for entity in ['action', 'location']:
    method_name = f"{entity}_q=10"
    train_df, dev_df, test_df = perform_split(df, entity, 10, method_name)
    mse = evaluate_distribution(df, train_df, dev_df, test_df, method_name, entity_columns)
    results[method_name] = mse

# Stratify on weighted sum of all entities (normalized weights based on total frequency of each entity)
entity_frequencies = df[entity_columns].sum()
total_freq = entity_frequencies.sum()
weights = entity_frequencies / total_freq
df['weighted_sum'] = (df[entity_columns] * weights).sum(axis=1)
method_name = "weighted_sum_all_q=10"
train_df, dev_df, test_df = perform_split(df, 'weighted_sum', 10, method_name)
mse = evaluate_distribution(df, train_df, dev_df, test_df, method_name, entity_columns)
results[method_name] = mse

for method, mse in sorted(results.items(), key=lambda x: x[1]):
    print(f"{method}: MSE = {mse:.6f}")

"""## Save locally"""

# Save the best split
# best_method = min(results, key=results.get)
# print(f"\nBest method: {best_method} with MSE = {results[best_method]:.6f}")

# if best_method.startswith("weighted_sum"):
#     stratify_col = "weighted_sum"
# else:
#     stratify_col = best_method.split('_')[0]

# n_bins = int(best_method.split('q=')[1] if 'q=' in best_method else 10)
# best_train_df, best_dev_df, best_test_df = perform_split(df, stratify_col, n_bins, best_method)

# best_train_df.to_csv('best_train_set.csv', index=False)
# best_dev_df.to_csv('best_dev_set.csv', index=False)
# best_test_df.to_csv('best_test_set.csv', index=False)

"""*   best_train_set.csv
*   best_dev_set.csv
*   best_test_set.csv
"""